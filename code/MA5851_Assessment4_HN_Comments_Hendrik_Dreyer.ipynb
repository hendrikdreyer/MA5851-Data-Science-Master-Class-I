{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Subject: MA5851 - Data Science Master Class I\n",
    "### Author: Hendrik A. Dreyer\n",
    "### Due Date: 3 December 2019\n",
    "\n",
    "#### This file contains the step to processing the Hacker News comments that are associated to the daily top 30 stories as posted during August 2019"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "script_dir = os.getcwd()  #<-- absolute dir the script is in\n",
    "rel_path = \"..\\data\\HN_BigQ_Results_Top30_Story_Comments_Aug_2019.csv\"\n",
    "abs_file_path = os.path.join(script_dir, rel_path)\n",
    "\n",
    "#Load the post process data from Assessment 3 into a pandas dataframe\n",
    "import pandas as pd\n",
    "\n",
    "# Control delimiters, rows, column names with read_csv (see later) \n",
    "data = pd.read_csv(abs_file_path) \n",
    "\n",
    "# Preview the first 5 lines of the loaded data \n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Shape of the loaded post-procssed data\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initiate spark\n",
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import the neccessary libs\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a spark session\n",
    "spark = SparkSession.builder.master('local[2]').appName('HN_Story_Comments').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Specify a structure onto which the RDD will be framed\n",
    "schema = StructType([\n",
    "    StructField(\"comment_date\", StringType(), True),\n",
    "    StructField(\"comment\",      StringType(), True)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the relative path to the pst process csv file\n",
    "spark_file_path = abs_file_path\n",
    "\n",
    "#Format the csv file for RDD ingestion\n",
    "data = spark.read.format(\"org.apache.spark.csv\")\\\n",
    "                        .option(\"delimiter\",\",\")\\\n",
    "                        .schema(schema)\\\n",
    "                        .option(\"mode\", \"PERMISSIVE\")\\\n",
    "                        .option(\"inferSchema\", \"True\")\\\n",
    "                        .csv(spark_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Have a quick peak\n",
    "data.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Filter the link_comment field and place int an RDD\n",
    "link_comment_rdd = data.select(\"comment\").rdd.flatMap(lambda x: x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "link_comment_rdd.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove the header from the rdd\n",
    "hn_header = link_comment_rdd.first()\n",
    "link_comments = link_comment_rdd.filter(lambda row: row != hn_header)\n",
    "\n",
    "link_comments.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convert the comments to lower case\n",
    "link_comments_lower = link_comments.map(lambda x : x.lower() if x is not None else x)\n",
    "\n",
    "link_comments_lower.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following section addresses the preparation of the corpus for the application of NLP techniques.  \n",
    "Each sentence in the corpus will be tokenized, follwed by the tokenization of each word in each tokenized sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the neccessary nltk libs before we start applying some NLP techniques\n",
    "import nltk\n",
    "from   nltk.corpus import stopwords\n",
    "from   nltk.stem   import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Make sure we take out all of the null values and empties in the RDD\n",
    "link_comments_lower_full = link_comments_lower.filter(lambda x: x is not None).filter(lambda x: x != \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tokenize the comment sentences and display\n",
    "def SententceTokenizer(x):\n",
    "    return nltk.sent_tokenize(x)\n",
    "\n",
    "#Apply function\n",
    "link_comments_tokenize = link_comments_lower_full.map(SententceTokenizer)\n",
    "\n",
    "#Have a peak\n",
    "link_comments_tokenize.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tokenize each word in the each title sentence \n",
    "def WordTokenizer(x):\n",
    "    token_words = [word for line in x for word in line.split()]\n",
    "    return token_words\n",
    "\n",
    "link_comments_tokenize_word = link_comments_tokenize.map(WordTokenizer)\n",
    "\n",
    "#Have another peal\n",
    "link_comments_tokenize_word.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "def removePunctuationsFunct(x):\n",
    "    list_punct=list(string.punctuation)\n",
    "    filtered = [''.join(c for c in s if c not in list_punct) for s in x] \n",
    "    filtered_space = [s for s in filtered if s] #remove empty space \n",
    "    return filtered\n",
    "#link_titles_tokenize_word\n",
    "#link_titles_punct = link_titles_clean.map(removePunctuationsFunct)\n",
    "link_comments_punct = link_comments_tokenize_word.map(removePunctuationsFunct)\n",
    "link_comments_punct.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now we apply lemmatization \n",
    "def lemmatizationFunct(x):\n",
    "    nltk.download('wordnet')\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    finalLem = [lemmatizer.lemmatize(s) for s in x]\n",
    "    return finalLem\n",
    "\n",
    "link_comments_lem = link_comments_punct.map(lemmatizationFunct)\n",
    "link_comments_lem.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Join the tokens together\n",
    "def JoinTokensTogether(val):\n",
    "    final_token_list = []\n",
    "    val = \" \".join(val)\n",
    "    return val\n",
    "\n",
    "link_comments_join = link_comments_lem.map(JoinTokensTogether)\n",
    "link_comments_join.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extract all the noun phrases\n",
    "def ExtractAllPhrases(x):\n",
    "    from nltk.corpus import stopwords\n",
    "    stop_words=set(stopwords.words('english'))    \n",
    "    \n",
    "    def leaves(tree):\n",
    "        \"\"\"Finds NP (nounphrase) leaf nodes of a chunk tree.\"\"\"\n",
    "        for subtree in tree.subtrees(filter = lambda t: t.label()=='NP'):\n",
    "            yield subtree.leaves()\n",
    "    \n",
    "    def get_terms(tree):\n",
    "        for leaf in leaves(tree):\n",
    "            term = [w for w,t in leaf if not w in stop_words]\n",
    "            yield term\n",
    "    \n",
    "    #Setup regular expression \n",
    "    sentence_regexp = r'(?:(?:[A-Z])(?:.[A-Z])+.?)|(?:\\w+(?:-\\w+)*)|(?:\\$?\\d+(?:.\\d+)?%?)|(?:...|)(?:[][.,;\"\\'?():-_`])'\n",
    "   \n",
    "    #Specify grammar rules\n",
    "    grammar_rule = r\"\"\"\n",
    "    NBAR:\n",
    "        {<NN.*|JJ>*<NN.*>}  # Nouns and Adjectives, terminated with Nouns\n",
    "        \n",
    "    NP:\n",
    "        {<NBAR>}\n",
    "        {<NBAR><IN><NBAR>}  # Above, connected with in/of/etc...\n",
    "    \"\"\"\n",
    "    \n",
    "    #Apply regexp parser\n",
    "    #Apply shallow parsing in order to give more context to the sentence\n",
    "    chunker = nltk.RegexpParser(grammar_rule)\n",
    "    tokens = nltk.regexp_tokenize(x,sentence_regexp)\n",
    "    \n",
    "    #Apply part of speech tagging\n",
    "    #Apply nous, verbs, adjectives, etc. \n",
    "    postoks = nltk.tag.pos_tag(tokens)  \n",
    "    \n",
    "    # Get a chink from the chunk\n",
    "    tree = chunker.parse(postoks) \n",
    "    terms = get_terms(tree)\n",
    "    temp_phrases = []\n",
    "    for term in terms:\n",
    "        if len(term):\n",
    "            temp_phrases.append(' '.join(term))\n",
    "            \n",
    "    #Get rid of the empty list\n",
    "    finalPhrase = [w for w in temp_phrases if w]\n",
    "    return finalPhrase\n",
    "\n",
    "\n",
    "link_comments_phrases = link_comments_join.map(ExtractAllPhrases)\n",
    "link_comments_phrases.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#To determine phrase sentiment values we'll use the “Valence Aware Dictionary and sEntiment Reasoner” - a.k.a VADER.\n",
    "import nltk\n",
    "nltk.download('vader_lexicon')\n",
    "\n",
    "def DetermineWordsSentiment(x):\n",
    "    from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "    analyzer = SentimentIntensityAnalyzer() \n",
    "    senti_list_temp = []    \n",
    "    for i in x:\n",
    "        y = ''.join(i) \n",
    "        vs = analyzer.polarity_scores(y)\n",
    "        senti_list_temp.append((y, vs))\n",
    "        senti_list_temp = [w for w in senti_list_temp if w]    \n",
    "    sentiment_list  = []\n",
    "    for j in senti_list_temp:\n",
    "        first = j[0]\n",
    "        second = j[1]\n",
    "        for (k,v) in second.items():\n",
    "            if k == 'compound':\n",
    "                if v < 0.0:\n",
    "                    sentiment_list.append((first, \"Negative\"))\n",
    "                elif v == 0.0:\n",
    "                    sentiment_list.append((first, \"Neutral\"))\n",
    "                else:\n",
    "                    sentiment_list.append((first, \"Positive\"))     \n",
    "    return sentiment_list\n",
    "\n",
    "sentimentRDD = link_comments_phrases.map(DetermineWordsSentiment)\n",
    "\n",
    "sentimentRDD.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Seperate out negative and positive sentiments\n",
    "def ExtractSentimentType(x,sentiment_type):\n",
    "    target_words = []\n",
    "    for items in x:\n",
    "        if len(items)>0:\n",
    "            if items[len(items)-1]== str(sentiment_type):\n",
    "                target_words.append(\" \".join(items[:len(items)-1]))\n",
    "    return target_words\n",
    "\n",
    "neg_sentiments = lambda x: ExtractSentimentType(x, \"Negative\")\n",
    "pos_sentiments = lambda x: ExtractSentimentType(x, \"Positive\")\n",
    "neg_link_titles_sentiment = sentimentRDD.map(neg_sentiments)\n",
    "pos_link_titles_sentiment = sentimentRDD.map(pos_sentiments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now let's extract the top 100 positive keywords from the extracted key phrases.\n",
    "freq_pos_link_titles = pos_link_titles_sentiment.flatMap(lambda x : nltk.FreqDist(x)\\\n",
    "                                            .most_common())\\\n",
    "                                            .map(lambda x: x)\\\n",
    "                                            .reduceByKey(lambda x,y : x+y)\\\n",
    "                                            .sortBy(lambda x: x[1], ascending = False)\n",
    "freq_pos_link_titles.take(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#converting RDD to spark dataframe\n",
    "df_fDist = freq_pos_link_titles.toDF()\n",
    "\n",
    "df_fDist.createOrReplaceTempView(\"myTable\") \n",
    "\n",
    "#renaming columns \n",
    "df2 = spark.sql(\"SELECT _1 AS Keywords, _2 as Frequency from myTable limit 20\") \n",
    "\n",
    " #converting spark dataframes to pandas dataframes\n",
    "pandD = df2.toPandas()\n",
    "pandD.plot.barh(x='Keywords', y='Frequency', rot=1, figsize=(10,8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now let's extract the top 100 negative keywords from the extracted key phrases.\n",
    "freq_neg_link_titles = neg_link_titles_sentiment.flatMap(lambda x : nltk.FreqDist(x)\\\n",
    "                                            .most_common())\\\n",
    "                                            .map(lambda x: x)\\\n",
    "                                            .reduceByKey(lambda x,y : x+y)\\\n",
    "                                            .sortBy(lambda x: x[1], ascending = False)\n",
    "freq_neg_link_titles.take(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#converting RDD to spark dataframe\n",
    "df_fDist = freq_neg_link_titles.toDF()\n",
    "\n",
    "df_fDist.createOrReplaceTempView(\"myTable\") \n",
    "\n",
    "#renaming columns \n",
    "df2 = spark.sql(\"SELECT _1 AS Keywords, _2 as Frequency from myTable limit 20\") \n",
    "\n",
    " #converting spark dataframes to pandas dataframes\n",
    "pandD = df2.toPandas()\n",
    "pandD.plot.barh(x='Keywords', y='Frequency', rot=1, figsize=(10,8))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
