{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Subject: MA5851 - Data Science Master Class I\n",
    "### Author: Hendrik A. Dreyer\n",
    "### Due Date: 3 December 2019\n",
    "\n",
    "#### This file contains the step to processing the Hacker News headlines as presentede by the given input file\n",
    "#### It then generates a sentiment histogram for the time frame's data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     C:\\Users\\driku\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[<matplotlib.axes._subplots.AxesSubplot object at 0x000001FD56730550>]],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "script_dir = os.getcwd()  #<-- absolute dir the script is in\n",
    "rel_path = \"..\\data\\hacker_news_post_process_2019.csv\"\n",
    "abs_file_path = os.path.join(script_dir, rel_path)\n",
    "\n",
    "#Load the post process data from Assessment 3 into a pandas dataframe\n",
    "import pandas as pd\n",
    "\n",
    "# Control delimiters, rows, column names with read_csv (see later) \n",
    "data = pd.read_csv(abs_file_path) \n",
    "\n",
    "#Initiate spark\n",
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "#Import the neccessary libs\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "#Create a spark session - 2 cores should be enough\n",
    "spark = SparkSession.builder.master('local[2]').appName('HN_Media_Titles').getOrCreate()\n",
    "\n",
    "#Specify a structure onto which the RDD will be framed\n",
    "schema = StructType([\n",
    "    StructField(\"id\",           StringType(),  True),\n",
    "    StructField(\"link_title\",   StringType(),  True),\n",
    "    StructField(\"web_link\",     StringType(),  True),\n",
    "    StructField(\"points\",       IntegerType(), True)])\n",
    "\n",
    "# Set the relative path to the pst process csv file\n",
    "spark_file_path = abs_file_path\n",
    "\n",
    "#Format the csv file for RDD ingestion\n",
    "data = spark.read.format(\"org.apache.spark.csv\")\\\n",
    "                        .option(\"delimiter\",\",\")\\\n",
    "                        .schema(schema)\\\n",
    "                        .option(\"mode\", \"PERMISSIVE\")\\\n",
    "                        .option(\"inferSchema\", \"True\")\\\n",
    "                        .csv(spark_file_path)\n",
    "\n",
    "#Filter the link_title field and place int an RDD\n",
    "link_title_rdd = data.select(\"link_title\").rdd.flatMap(lambda x: x)\n",
    "\n",
    "#Remove the header from the rdd\n",
    "hn_header = link_title_rdd.first()\n",
    "link_titles = link_title_rdd.filter(lambda row: row != hn_header)\n",
    "\n",
    "# Load the neccessary nltk libs before we start applying some NLP techniques\n",
    "import nltk\n",
    "from   nltk.corpus import stopwords\n",
    "from   nltk.stem   import WordNetLemmatizer\n",
    "\n",
    "#Tokenize the title sentences and display\n",
    "def SententceTokenizer(x):\n",
    "    return nltk.sent_tokenize(x)\n",
    "\n",
    "#Perform Sentence Tokenisation\n",
    "link_titles_tokenize = link_titles.map(SententceTokenizer)\n",
    "\n",
    "#To determine phrase sentiment values we'll use the “Valence Aware Dictionary and sEntiment Reasoner” - a.k.a VADER.\n",
    "import nltk\n",
    "nltk.download('vader_lexicon')\n",
    "\n",
    "def DetermineHeadingSentiment(x):\n",
    "    from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "    analyzer = SentimentIntensityAnalyzer() \n",
    "    senti_list_temp = []    \n",
    "    for i in x:\n",
    "        y = ''.join(i) \n",
    "        vs = analyzer.polarity_scores(y)\n",
    "        senti_list_temp.append((y, vs))\n",
    "        senti_list_temp = [w for w in senti_list_temp if w]    \n",
    "    sentiment_list  = []\n",
    "    for j in senti_list_temp:\n",
    "        first = j[0]\n",
    "        second = j[1]\n",
    "        for (k,v) in second.items():\n",
    "            if k == 'compound':\n",
    "                sentiment_list.append((first,v))  \n",
    "    return sentiment_list    \n",
    "\n",
    "sentimentRDD = link_titles_tokenize.map(DetermineHeadingSentiment)\n",
    "\n",
    "#Flatten the RDD.\n",
    "flat_link_titles = sentimentRDD.flatMap(lambda x :x)\n",
    "\n",
    "#converting RDD to spark dataframe\n",
    "spark_df_headings = flat_link_titles.toDF()\n",
    "\n",
    "spark_df_headings.createOrReplaceTempView(\"TableHeadings\") \n",
    "\n",
    "#renaming columns \n",
    "spark_df_tmp = spark.sql(\"SELECT _1 AS Keywords, _2 as Headings_Sentiment from TableHeadings\") \n",
    "\n",
    "#Converting spark dataframes to pandas dataframes\n",
    "headings_df = spark_df_tmp.toPandas()\n",
    "\n",
    "#Plot a histogram\n",
    "headings_df.hist('Headings_Sentiment', bins = 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:py4j.java_gateway:An error occurred while trying to connect to the Java server (127.0.0.1:50587)\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\driku\\OneDrive\\Documents\\Dev\\spark-2.4.4-bin-hadoop2.7\\python\\lib\\py4j-0.10.7-src.zip\\py4j\\java_gateway.py\", line 929, in _get_connection\n",
      "    connection = self.deque.pop()\n",
      "IndexError: pop from an empty deque\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\driku\\OneDrive\\Documents\\Dev\\spark-2.4.4-bin-hadoop2.7\\python\\lib\\py4j-0.10.7-src.zip\\py4j\\java_gateway.py\", line 1067, in start\n",
      "    self.socket.connect((self.address, self.port))\n",
      "ConnectionRefusedError: [WinError 10061] No connection could be made because the target machine actively refused it\n",
      "ERROR:py4j.java_gateway:An error occurred while trying to connect to the Java server (127.0.0.1:50587)\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\driku\\OneDrive\\Documents\\Dev\\spark-2.4.4-bin-hadoop2.7\\python\\lib\\py4j-0.10.7-src.zip\\py4j\\java_gateway.py\", line 929, in _get_connection\n",
      "    connection = self.deque.pop()\n",
      "IndexError: pop from an empty deque\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\driku\\OneDrive\\Documents\\Dev\\spark-2.4.4-bin-hadoop2.7\\python\\lib\\py4j-0.10.7-src.zip\\py4j\\java_gateway.py\", line 1067, in start\n",
      "    self.socket.connect((self.address, self.port))\n",
      "ConnectionRefusedError: [WinError 10061] No connection could be made because the target machine actively refused it\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\driku\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3296, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"<ipython-input-9-cac0ddd4aceb>\", line 21, in <module>\n",
      "    spark = SparkSession.builder.master('local[2]').appName('HN_Media_Titles').getOrCreate()\n",
      "  File \"C:\\Users\\driku\\OneDrive\\Documents\\Dev\\spark-2.4.4-bin-hadoop2.7\\python\\pyspark\\sql\\session.py\", line 183, in getOrCreate\n",
      "    session._jsparkSession.sessionState().conf().setConfString(key, value)\n",
      "  File \"C:\\Users\\driku\\OneDrive\\Documents\\Dev\\spark-2.4.4-bin-hadoop2.7\\python\\lib\\py4j-0.10.7-src.zip\\py4j\\java_gateway.py\", line 1255, in __call__\n",
      "    answer = self.gateway_client.send_command(command)\n",
      "  File \"C:\\Users\\driku\\OneDrive\\Documents\\Dev\\spark-2.4.4-bin-hadoop2.7\\python\\lib\\py4j-0.10.7-src.zip\\py4j\\java_gateway.py\", line 983, in send_command\n",
      "    connection = self._get_connection()\n",
      "  File \"C:\\Users\\driku\\OneDrive\\Documents\\Dev\\spark-2.4.4-bin-hadoop2.7\\python\\lib\\py4j-0.10.7-src.zip\\py4j\\java_gateway.py\", line 931, in _get_connection\n",
      "    connection = self._create_connection()\n",
      "  File \"C:\\Users\\driku\\OneDrive\\Documents\\Dev\\spark-2.4.4-bin-hadoop2.7\\python\\lib\\py4j-0.10.7-src.zip\\py4j\\java_gateway.py\", line 937, in _create_connection\n",
      "    connection.start()\n",
      "  File \"C:\\Users\\driku\\OneDrive\\Documents\\Dev\\spark-2.4.4-bin-hadoop2.7\\python\\lib\\py4j-0.10.7-src.zip\\py4j\\java_gateway.py\", line 1079, in start\n",
      "    raise Py4JNetworkError(msg, e)\n",
      "py4j.protocol.Py4JNetworkError: An error occurred while trying to connect to the Java server (127.0.0.1:50587)\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\driku\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2033, in showtraceback\n",
      "    stb = value._render_traceback_()\n",
      "AttributeError: 'Py4JNetworkError' object has no attribute '_render_traceback_'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\driku\\OneDrive\\Documents\\Dev\\spark-2.4.4-bin-hadoop2.7\\python\\lib\\py4j-0.10.7-src.zip\\py4j\\java_gateway.py\", line 929, in _get_connection\n",
      "    connection = self.deque.pop()\n",
      "IndexError: pop from an empty deque\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\driku\\OneDrive\\Documents\\Dev\\spark-2.4.4-bin-hadoop2.7\\python\\lib\\py4j-0.10.7-src.zip\\py4j\\java_gateway.py\", line 1067, in start\n",
      "    self.socket.connect((self.address, self.port))\n",
      "ConnectionRefusedError: [WinError 10061] No connection could be made because the target machine actively refused it\n",
      "ERROR:py4j.java_gateway:An error occurred while trying to connect to the Java server (127.0.0.1:50587)\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\driku\\OneDrive\\Documents\\Dev\\spark-2.4.4-bin-hadoop2.7\\python\\lib\\py4j-0.10.7-src.zip\\py4j\\java_gateway.py\", line 929, in _get_connection\n",
      "    connection = self.deque.pop()\n",
      "IndexError: pop from an empty deque\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\driku\\OneDrive\\Documents\\Dev\\spark-2.4.4-bin-hadoop2.7\\python\\lib\\py4j-0.10.7-src.zip\\py4j\\java_gateway.py\", line 1067, in start\n",
      "    self.socket.connect((self.address, self.port))\n",
      "ConnectionRefusedError: [WinError 10061] No connection could be made because the target machine actively refused it\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\driku\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3296, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"<ipython-input-9-cac0ddd4aceb>\", line 21, in <module>\n",
      "    spark = SparkSession.builder.master('local[2]').appName('HN_Media_Titles').getOrCreate()\n",
      "  File \"C:\\Users\\driku\\OneDrive\\Documents\\Dev\\spark-2.4.4-bin-hadoop2.7\\python\\pyspark\\sql\\session.py\", line 183, in getOrCreate\n",
      "    session._jsparkSession.sessionState().conf().setConfString(key, value)\n",
      "  File \"C:\\Users\\driku\\OneDrive\\Documents\\Dev\\spark-2.4.4-bin-hadoop2.7\\python\\lib\\py4j-0.10.7-src.zip\\py4j\\java_gateway.py\", line 1255, in __call__\n",
      "    answer = self.gateway_client.send_command(command)\n",
      "  File \"C:\\Users\\driku\\OneDrive\\Documents\\Dev\\spark-2.4.4-bin-hadoop2.7\\python\\lib\\py4j-0.10.7-src.zip\\py4j\\java_gateway.py\", line 983, in send_command\n",
      "    connection = self._get_connection()\n",
      "  File \"C:\\Users\\driku\\OneDrive\\Documents\\Dev\\spark-2.4.4-bin-hadoop2.7\\python\\lib\\py4j-0.10.7-src.zip\\py4j\\java_gateway.py\", line 931, in _get_connection\n",
      "    connection = self._create_connection()\n",
      "  File \"C:\\Users\\driku\\OneDrive\\Documents\\Dev\\spark-2.4.4-bin-hadoop2.7\\python\\lib\\py4j-0.10.7-src.zip\\py4j\\java_gateway.py\", line 937, in _create_connection\n",
      "    connection.start()\n",
      "  File \"C:\\Users\\driku\\OneDrive\\Documents\\Dev\\spark-2.4.4-bin-hadoop2.7\\python\\lib\\py4j-0.10.7-src.zip\\py4j\\java_gateway.py\", line 1079, in start\n",
      "    raise Py4JNetworkError(msg, e)\n",
      "py4j.protocol.Py4JNetworkError: An error occurred while trying to connect to the Java server (127.0.0.1:50587)\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\driku\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2033, in showtraceback\n",
      "    stb = value._render_traceback_()\n",
      "AttributeError: 'Py4JNetworkError' object has no attribute '_render_traceback_'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\driku\\OneDrive\\Documents\\Dev\\spark-2.4.4-bin-hadoop2.7\\python\\lib\\py4j-0.10.7-src.zip\\py4j\\java_gateway.py\", line 929, in _get_connection\n",
      "    connection = self.deque.pop()\n",
      "IndexError: pop from an empty deque\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\driku\\OneDrive\\Documents\\Dev\\spark-2.4.4-bin-hadoop2.7\\python\\lib\\py4j-0.10.7-src.zip\\py4j\\java_gateway.py\", line 1067, in start\n",
      "    self.socket.connect((self.address, self.port))\n",
      "ConnectionRefusedError: [WinError 10061] No connection could be made because the target machine actively refused it\n",
      "ERROR:py4j.java_gateway:An error occurred while trying to connect to the Java server (127.0.0.1:50587)\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\driku\\OneDrive\\Documents\\Dev\\spark-2.4.4-bin-hadoop2.7\\python\\lib\\py4j-0.10.7-src.zip\\py4j\\java_gateway.py\", line 929, in _get_connection\n",
      "    connection = self.deque.pop()\n",
      "IndexError: pop from an empty deque\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\driku\\OneDrive\\Documents\\Dev\\spark-2.4.4-bin-hadoop2.7\\python\\lib\\py4j-0.10.7-src.zip\\py4j\\java_gateway.py\", line 1067, in start\n",
      "    self.socket.connect((self.address, self.port))\n",
      "ConnectionRefusedError: [WinError 10061] No connection could be made because the target machine actively refused it\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\driku\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3296, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"<ipython-input-9-cac0ddd4aceb>\", line 21, in <module>\n",
      "    spark = SparkSession.builder.master('local[2]').appName('HN_Media_Titles').getOrCreate()\n",
      "  File \"C:\\Users\\driku\\OneDrive\\Documents\\Dev\\spark-2.4.4-bin-hadoop2.7\\python\\pyspark\\sql\\session.py\", line 183, in getOrCreate\n",
      "    session._jsparkSession.sessionState().conf().setConfString(key, value)\n",
      "  File \"C:\\Users\\driku\\OneDrive\\Documents\\Dev\\spark-2.4.4-bin-hadoop2.7\\python\\lib\\py4j-0.10.7-src.zip\\py4j\\java_gateway.py\", line 1255, in __call__\n",
      "    answer = self.gateway_client.send_command(command)\n",
      "  File \"C:\\Users\\driku\\OneDrive\\Documents\\Dev\\spark-2.4.4-bin-hadoop2.7\\python\\lib\\py4j-0.10.7-src.zip\\py4j\\java_gateway.py\", line 983, in send_command\n",
      "    connection = self._get_connection()\n",
      "  File \"C:\\Users\\driku\\OneDrive\\Documents\\Dev\\spark-2.4.4-bin-hadoop2.7\\python\\lib\\py4j-0.10.7-src.zip\\py4j\\java_gateway.py\", line 931, in _get_connection\n",
      "    connection = self._create_connection()\n",
      "  File \"C:\\Users\\driku\\OneDrive\\Documents\\Dev\\spark-2.4.4-bin-hadoop2.7\\python\\lib\\py4j-0.10.7-src.zip\\py4j\\java_gateway.py\", line 937, in _create_connection\n",
      "    connection.start()\n",
      "  File \"C:\\Users\\driku\\OneDrive\\Documents\\Dev\\spark-2.4.4-bin-hadoop2.7\\python\\lib\\py4j-0.10.7-src.zip\\py4j\\java_gateway.py\", line 1079, in start\n",
      "    raise Py4JNetworkError(msg, e)\n",
      "py4j.protocol.Py4JNetworkError: An error occurred while trying to connect to the Java server (127.0.0.1:50587)\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\driku\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2033, in showtraceback\n",
      "    stb = value._render_traceback_()\n",
      "AttributeError: 'Py4JNetworkError' object has no attribute '_render_traceback_'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\driku\\OneDrive\\Documents\\Dev\\spark-2.4.4-bin-hadoop2.7\\python\\lib\\py4j-0.10.7-src.zip\\py4j\\java_gateway.py\", line 929, in _get_connection\n",
      "    connection = self.deque.pop()\n",
      "IndexError: pop from an empty deque\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\driku\\OneDrive\\Documents\\Dev\\spark-2.4.4-bin-hadoop2.7\\python\\lib\\py4j-0.10.7-src.zip\\py4j\\java_gateway.py\", line 1067, in start\n",
      "    self.socket.connect((self.address, self.port))\n",
      "ConnectionRefusedError: [WinError 10061] No connection could be made because the target machine actively refused it\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:py4j.java_gateway:An error occurred while trying to connect to the Java server (127.0.0.1:50587)\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\driku\\OneDrive\\Documents\\Dev\\spark-2.4.4-bin-hadoop2.7\\python\\lib\\py4j-0.10.7-src.zip\\py4j\\java_gateway.py\", line 929, in _get_connection\n",
      "    connection = self.deque.pop()\n",
      "IndexError: pop from an empty deque\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\driku\\OneDrive\\Documents\\Dev\\spark-2.4.4-bin-hadoop2.7\\python\\lib\\py4j-0.10.7-src.zip\\py4j\\java_gateway.py\", line 1067, in start\n",
      "    self.socket.connect((self.address, self.port))\n",
      "ConnectionRefusedError: [WinError 10061] No connection could be made because the target machine actively refused it\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\driku\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3296, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"<ipython-input-9-cac0ddd4aceb>\", line 21, in <module>\n",
      "    spark = SparkSession.builder.master('local[2]').appName('HN_Media_Titles').getOrCreate()\n",
      "  File \"C:\\Users\\driku\\OneDrive\\Documents\\Dev\\spark-2.4.4-bin-hadoop2.7\\python\\pyspark\\sql\\session.py\", line 183, in getOrCreate\n",
      "    session._jsparkSession.sessionState().conf().setConfString(key, value)\n",
      "  File \"C:\\Users\\driku\\OneDrive\\Documents\\Dev\\spark-2.4.4-bin-hadoop2.7\\python\\lib\\py4j-0.10.7-src.zip\\py4j\\java_gateway.py\", line 1255, in __call__\n",
      "    answer = self.gateway_client.send_command(command)\n",
      "  File \"C:\\Users\\driku\\OneDrive\\Documents\\Dev\\spark-2.4.4-bin-hadoop2.7\\python\\lib\\py4j-0.10.7-src.zip\\py4j\\java_gateway.py\", line 983, in send_command\n",
      "    connection = self._get_connection()\n",
      "  File \"C:\\Users\\driku\\OneDrive\\Documents\\Dev\\spark-2.4.4-bin-hadoop2.7\\python\\lib\\py4j-0.10.7-src.zip\\py4j\\java_gateway.py\", line 931, in _get_connection\n",
      "    connection = self._create_connection()\n",
      "  File \"C:\\Users\\driku\\OneDrive\\Documents\\Dev\\spark-2.4.4-bin-hadoop2.7\\python\\lib\\py4j-0.10.7-src.zip\\py4j\\java_gateway.py\", line 937, in _create_connection\n",
      "    connection.start()\n",
      "  File \"C:\\Users\\driku\\OneDrive\\Documents\\Dev\\spark-2.4.4-bin-hadoop2.7\\python\\lib\\py4j-0.10.7-src.zip\\py4j\\java_gateway.py\", line 1079, in start\n",
      "    raise Py4JNetworkError(msg, e)\n",
      "py4j.protocol.Py4JNetworkError: An error occurred while trying to connect to the Java server (127.0.0.1:50587)\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\driku\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2033, in showtraceback\n",
      "    stb = value._render_traceback_()\n",
      "AttributeError: 'Py4JNetworkError' object has no attribute '_render_traceback_'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\driku\\OneDrive\\Documents\\Dev\\spark-2.4.4-bin-hadoop2.7\\python\\lib\\py4j-0.10.7-src.zip\\py4j\\java_gateway.py\", line 929, in _get_connection\n",
      "    connection = self.deque.pop()\n",
      "IndexError: pop from an empty deque\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\driku\\OneDrive\\Documents\\Dev\\spark-2.4.4-bin-hadoop2.7\\python\\lib\\py4j-0.10.7-src.zip\\py4j\\java_gateway.py\", line 1067, in start\n",
      "    self.socket.connect((self.address, self.port))\n",
      "ConnectionRefusedError: [WinError 10061] No connection could be made because the target machine actively refused it\n",
      "ERROR:py4j.java_gateway:An error occurred while trying to connect to the Java server (127.0.0.1:50587)\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\driku\\OneDrive\\Documents\\Dev\\spark-2.4.4-bin-hadoop2.7\\python\\lib\\py4j-0.10.7-src.zip\\py4j\\java_gateway.py\", line 929, in _get_connection\n",
      "    connection = self.deque.pop()\n",
      "IndexError: pop from an empty deque\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\driku\\OneDrive\\Documents\\Dev\\spark-2.4.4-bin-hadoop2.7\\python\\lib\\py4j-0.10.7-src.zip\\py4j\\java_gateway.py\", line 1067, in start\n",
      "    self.socket.connect((self.address, self.port))\n",
      "ConnectionRefusedError: [WinError 10061] No connection could be made because the target machine actively refused it\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\driku\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3296, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"<ipython-input-9-cac0ddd4aceb>\", line 21, in <module>\n",
      "    spark = SparkSession.builder.master('local[2]').appName('HN_Media_Titles').getOrCreate()\n",
      "  File \"C:\\Users\\driku\\OneDrive\\Documents\\Dev\\spark-2.4.4-bin-hadoop2.7\\python\\pyspark\\sql\\session.py\", line 183, in getOrCreate\n",
      "    session._jsparkSession.sessionState().conf().setConfString(key, value)\n",
      "  File \"C:\\Users\\driku\\OneDrive\\Documents\\Dev\\spark-2.4.4-bin-hadoop2.7\\python\\lib\\py4j-0.10.7-src.zip\\py4j\\java_gateway.py\", line 1255, in __call__\n",
      "    answer = self.gateway_client.send_command(command)\n",
      "  File \"C:\\Users\\driku\\OneDrive\\Documents\\Dev\\spark-2.4.4-bin-hadoop2.7\\python\\lib\\py4j-0.10.7-src.zip\\py4j\\java_gateway.py\", line 983, in send_command\n",
      "    connection = self._get_connection()\n",
      "  File \"C:\\Users\\driku\\OneDrive\\Documents\\Dev\\spark-2.4.4-bin-hadoop2.7\\python\\lib\\py4j-0.10.7-src.zip\\py4j\\java_gateway.py\", line 931, in _get_connection\n",
      "    connection = self._create_connection()\n",
      "  File \"C:\\Users\\driku\\OneDrive\\Documents\\Dev\\spark-2.4.4-bin-hadoop2.7\\python\\lib\\py4j-0.10.7-src.zip\\py4j\\java_gateway.py\", line 937, in _create_connection\n",
      "    connection.start()\n",
      "  File \"C:\\Users\\driku\\OneDrive\\Documents\\Dev\\spark-2.4.4-bin-hadoop2.7\\python\\lib\\py4j-0.10.7-src.zip\\py4j\\java_gateway.py\", line 1079, in start\n",
      "    raise Py4JNetworkError(msg, e)\n",
      "py4j.protocol.Py4JNetworkError: An error occurred while trying to connect to the Java server (127.0.0.1:50587)\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\driku\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2033, in showtraceback\n",
      "    stb = value._render_traceback_()\n",
      "AttributeError: 'Py4JNetworkError' object has no attribute '_render_traceback_'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\driku\\OneDrive\\Documents\\Dev\\spark-2.4.4-bin-hadoop2.7\\python\\lib\\py4j-0.10.7-src.zip\\py4j\\java_gateway.py\", line 929, in _get_connection\n",
      "    connection = self.deque.pop()\n",
      "IndexError: pop from an empty deque\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\driku\\OneDrive\\Documents\\Dev\\spark-2.4.4-bin-hadoop2.7\\python\\lib\\py4j-0.10.7-src.zip\\py4j\\java_gateway.py\", line 1067, in start\n",
      "    self.socket.connect((self.address, self.port))\n",
      "ConnectionRefusedError: [WinError 10061] No connection could be made because the target machine actively refused it\n",
      "ERROR:py4j.java_gateway:An error occurred while trying to connect to the Java server (127.0.0.1:50587)\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\driku\\OneDrive\\Documents\\Dev\\spark-2.4.4-bin-hadoop2.7\\python\\lib\\py4j-0.10.7-src.zip\\py4j\\java_gateway.py\", line 929, in _get_connection\n",
      "    connection = self.deque.pop()\n",
      "IndexError: pop from an empty deque\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\driku\\OneDrive\\Documents\\Dev\\spark-2.4.4-bin-hadoop2.7\\python\\lib\\py4j-0.10.7-src.zip\\py4j\\java_gateway.py\", line 1067, in start\n",
      "    self.socket.connect((self.address, self.port))\n",
      "ConnectionRefusedError: [WinError 10061] No connection could be made because the target machine actively refused it\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\driku\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3296, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"<ipython-input-9-cac0ddd4aceb>\", line 21, in <module>\n",
      "    spark = SparkSession.builder.master('local[2]').appName('HN_Media_Titles').getOrCreate()\n",
      "  File \"C:\\Users\\driku\\OneDrive\\Documents\\Dev\\spark-2.4.4-bin-hadoop2.7\\python\\pyspark\\sql\\session.py\", line 183, in getOrCreate\n",
      "    session._jsparkSession.sessionState().conf().setConfString(key, value)\n",
      "  File \"C:\\Users\\driku\\OneDrive\\Documents\\Dev\\spark-2.4.4-bin-hadoop2.7\\python\\lib\\py4j-0.10.7-src.zip\\py4j\\java_gateway.py\", line 1255, in __call__\n",
      "    answer = self.gateway_client.send_command(command)\n",
      "  File \"C:\\Users\\driku\\OneDrive\\Documents\\Dev\\spark-2.4.4-bin-hadoop2.7\\python\\lib\\py4j-0.10.7-src.zip\\py4j\\java_gateway.py\", line 983, in send_command\n",
      "    connection = self._get_connection()\n",
      "  File \"C:\\Users\\driku\\OneDrive\\Documents\\Dev\\spark-2.4.4-bin-hadoop2.7\\python\\lib\\py4j-0.10.7-src.zip\\py4j\\java_gateway.py\", line 931, in _get_connection\n",
      "    connection = self._create_connection()\n",
      "  File \"C:\\Users\\driku\\OneDrive\\Documents\\Dev\\spark-2.4.4-bin-hadoop2.7\\python\\lib\\py4j-0.10.7-src.zip\\py4j\\java_gateway.py\", line 937, in _create_connection\n",
      "    connection.start()\n",
      "  File \"C:\\Users\\driku\\OneDrive\\Documents\\Dev\\spark-2.4.4-bin-hadoop2.7\\python\\lib\\py4j-0.10.7-src.zip\\py4j\\java_gateway.py\", line 1079, in start\n",
      "    raise Py4JNetworkError(msg, e)\n",
      "py4j.protocol.Py4JNetworkError: An error occurred while trying to connect to the Java server (127.0.0.1:50587)\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\driku\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2033, in showtraceback\n",
      "    stb = value._render_traceback_()\n",
      "AttributeError: 'Py4JNetworkError' object has no attribute '_render_traceback_'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\driku\\OneDrive\\Documents\\Dev\\spark-2.4.4-bin-hadoop2.7\\python\\lib\\py4j-0.10.7-src.zip\\py4j\\java_gateway.py\", line 929, in _get_connection\n",
      "    connection = self.deque.pop()\n",
      "IndexError: pop from an empty deque\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\driku\\OneDrive\\Documents\\Dev\\spark-2.4.4-bin-hadoop2.7\\python\\lib\\py4j-0.10.7-src.zip\\py4j\\java_gateway.py\", line 1067, in start\n",
      "    self.socket.connect((self.address, self.port))\n",
      "ConnectionRefusedError: [WinError 10061] No connection could be made because the target machine actively refused it\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:py4j.java_gateway:An error occurred while trying to connect to the Java server (127.0.0.1:50587)\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\driku\\OneDrive\\Documents\\Dev\\spark-2.4.4-bin-hadoop2.7\\python\\lib\\py4j-0.10.7-src.zip\\py4j\\java_gateway.py\", line 929, in _get_connection\n",
      "    connection = self.deque.pop()\n",
      "IndexError: pop from an empty deque\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\driku\\OneDrive\\Documents\\Dev\\spark-2.4.4-bin-hadoop2.7\\python\\lib\\py4j-0.10.7-src.zip\\py4j\\java_gateway.py\", line 1067, in start\n",
      "    self.socket.connect((self.address, self.port))\n",
      "ConnectionRefusedError: [WinError 10061] No connection could be made because the target machine actively refused it\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\driku\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3296, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"<ipython-input-9-cac0ddd4aceb>\", line 21, in <module>\n",
      "    spark = SparkSession.builder.master('local[2]').appName('HN_Media_Titles').getOrCreate()\n",
      "  File \"C:\\Users\\driku\\OneDrive\\Documents\\Dev\\spark-2.4.4-bin-hadoop2.7\\python\\pyspark\\sql\\session.py\", line 183, in getOrCreate\n",
      "    session._jsparkSession.sessionState().conf().setConfString(key, value)\n",
      "  File \"C:\\Users\\driku\\OneDrive\\Documents\\Dev\\spark-2.4.4-bin-hadoop2.7\\python\\lib\\py4j-0.10.7-src.zip\\py4j\\java_gateway.py\", line 1255, in __call__\n",
      "    answer = self.gateway_client.send_command(command)\n",
      "  File \"C:\\Users\\driku\\OneDrive\\Documents\\Dev\\spark-2.4.4-bin-hadoop2.7\\python\\lib\\py4j-0.10.7-src.zip\\py4j\\java_gateway.py\", line 983, in send_command\n",
      "    connection = self._get_connection()\n",
      "  File \"C:\\Users\\driku\\OneDrive\\Documents\\Dev\\spark-2.4.4-bin-hadoop2.7\\python\\lib\\py4j-0.10.7-src.zip\\py4j\\java_gateway.py\", line 931, in _get_connection\n",
      "    connection = self._create_connection()\n",
      "  File \"C:\\Users\\driku\\OneDrive\\Documents\\Dev\\spark-2.4.4-bin-hadoop2.7\\python\\lib\\py4j-0.10.7-src.zip\\py4j\\java_gateway.py\", line 937, in _create_connection\n",
      "    connection.start()\n",
      "  File \"C:\\Users\\driku\\OneDrive\\Documents\\Dev\\spark-2.4.4-bin-hadoop2.7\\python\\lib\\py4j-0.10.7-src.zip\\py4j\\java_gateway.py\", line 1079, in start\n",
      "    raise Py4JNetworkError(msg, e)\n",
      "py4j.protocol.Py4JNetworkError: An error occurred while trying to connect to the Java server (127.0.0.1:50587)\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\driku\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2033, in showtraceback\n",
      "    stb = value._render_traceback_()\n",
      "AttributeError: 'Py4JNetworkError' object has no attribute '_render_traceback_'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\driku\\OneDrive\\Documents\\Dev\\spark-2.4.4-bin-hadoop2.7\\python\\lib\\py4j-0.10.7-src.zip\\py4j\\java_gateway.py\", line 929, in _get_connection\n",
      "    connection = self.deque.pop()\n",
      "IndexError: pop from an empty deque\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\driku\\OneDrive\\Documents\\Dev\\spark-2.4.4-bin-hadoop2.7\\python\\lib\\py4j-0.10.7-src.zip\\py4j\\java_gateway.py\", line 1067, in start\n",
      "    self.socket.connect((self.address, self.port))\n",
      "ConnectionRefusedError: [WinError 10061] No connection could be made because the target machine actively refused it\n"
     ]
    }
   ],
   "source": [
    "script_dir = os.getcwd()  #<-- absolute dir the script is in\n",
    "rel_path = \"..\\data\\Week_1_Aug_2019_Comments.csv\"\n",
    "abs_file_path_comments = os.path.join(script_dir, rel_path)\n",
    "\n",
    "#Load the post process data from Assessment 3 into a pandas dataframe\n",
    "import pandas as pd\n",
    "\n",
    "# Control delimiters, rows, column names with read_csv (see later) \n",
    "data = pd.read_csv(abs_file_path_comments) \n",
    "\n",
    "#Initiate spark\n",
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "#Import the neccessary libs\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "#Create a spark session - 2 cores should be enough\n",
    "spark = SparkSession.builder.master('local[2]').appName('HN_Media_Titles').getOrCreate()\n",
    "\n",
    "#Specify a structure onto which the RDD will be framed\n",
    "schema = StructType([StructField(\"text\", StringType(),  True)])\n",
    "\n",
    "# Set the relative path to the pst process csv file\n",
    "spark_file_path = abs_file_path_comments\n",
    "\n",
    "#Format the csv file for RDD ingestion\n",
    "data = spark.read.format(\"org.apache.spark.csv\")\\\n",
    "                        .option(\"delimiter\",\",\")\\\n",
    "                        .schema(schema)\\\n",
    "                        .option(\"mode\", \"PERMISSIVE\")\\\n",
    "                        .option(\"inferSchema\", \"True\")\\\n",
    "                        .csv(spark_file_path)\n",
    "\n",
    "#Filter the link_title field and place int an RDD\n",
    "link_comment_rdd = data.select(\"text\").rdd.flatMap(lambda x: x)\n",
    "\n",
    "#Remove the header from the rdd\n",
    "hn_header = link_comment_rdd.first()\n",
    "link_comments = link_comment_rdd.filter(lambda row: row != hn_header)\n",
    "\n",
    "#Tokenize the comment sentences and display\n",
    "def SententceTokenizer(x):\n",
    "    return nltk.sent_tokenize(x)\n",
    "\n",
    "#Apply function\n",
    "link_comments_tokenize = link_comments.map(SententceTokenizer)\n",
    "\n",
    "fixed_html_comments = []\n",
    "\n",
    "def ParseHTMLTokens(x):\n",
    "    from html.parser import HTMLParser\n",
    "    h = HTMLParser()\n",
    "    for item in x:\n",
    "        fixed_html_comments.append(h.unescape(x))\n",
    "    return fixed_html_comments\n",
    "\n",
    "parsed_comments = link_comments_tokenize.map(ParseHTMLTokens)\n",
    "\n",
    "\n",
    "#Have a peak\n",
    "link_comments_tokenize.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove the header from the rdd\n",
    "hn_header = link_title_rdd.first()\n",
    "link_titles = link_title_rdd.filter(lambda row: row != hn_header)\n",
    "\n",
    "# Load the neccessary nltk libs before we start applying some NLP techniques\n",
    "import nltk\n",
    "from   nltk.corpus import stopwords\n",
    "from   nltk.stem   import WordNetLemmatizer\n",
    "\n",
    "#Tokenize the title sentences and display\n",
    "def SententceTokenizer(x):\n",
    "    return nltk.sent_tokenize(x)\n",
    "\n",
    "#Perform Sentence Tokenisation\n",
    "link_titles_tokenize = link_titles.map(SententceTokenizer)\n",
    "\n",
    "#To determine phrase sentiment values we'll use the “Valence Aware Dictionary and sEntiment Reasoner” - a.k.a VADER.\n",
    "import nltk\n",
    "nltk.download('vader_lexicon')\n",
    "\n",
    "def DetermineHeadingSentiment(x):\n",
    "    from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "    analyzer = SentimentIntensityAnalyzer() \n",
    "    senti_list_temp = []    \n",
    "    for i in x:\n",
    "        y = ''.join(i) \n",
    "        vs = analyzer.polarity_scores(y)\n",
    "        senti_list_temp.append((y, vs))\n",
    "        senti_list_temp = [w for w in senti_list_temp if w]    \n",
    "    sentiment_list  = []\n",
    "    for j in senti_list_temp:\n",
    "        first = j[0]\n",
    "        second = j[1]\n",
    "        for (k,v) in second.items():\n",
    "            if k == 'compound':\n",
    "                sentiment_list.append((first,v))  \n",
    "    return sentiment_list    \n",
    "\n",
    "sentimentRDD = link_titles_tokenize.map(DetermineHeadingSentiment)\n",
    "\n",
    "#Flatten the RDD.\n",
    "#flat_link_titles = sentimentRDD.flatMap(lambda x :x)\n",
    "\n",
    "#converting RDD to spark dataframe\n",
    "#spark_df_headings = flat_link_titles.toDF()\n",
    "\n",
    "#spark_df_headings.createOrReplaceTempView(\"TableHeadings\") \n",
    "\n",
    "#renaming columns \n",
    "#spark_df_tmp = spark.sql(\"SELECT _1 AS Keywords, _2 as Headings_Sentiment from TableHeadings\") \n",
    "\n",
    "#Converting spark dataframes to pandas dataframes\n",
    "#headings_df = spark_df_tmp.toPandas()\n",
    "\n",
    "#Plot a histogram\n",
    "#headings_df.hist('Headings_Sentiment', bins = 20)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
